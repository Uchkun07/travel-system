"""
ä¸­å›½çŸ¥åæ™¯ç‚¹çˆ¬è™«ç¨‹åº
æ•°æ®æºï¼š
1. ç»´åŸºç™¾ç§‘ - ä¸­å›½ä¸–ç•Œé—äº§åˆ—è¡¨
2. ç»´åŸºç™¾ç§‘ - ä¸­å›½5Açº§æ™¯åŒºåˆ—è¡¨
3. ç»´åŸºç™¾ç§‘ - ä¸­å›½å„çœæ—…æ¸¸æ™¯ç‚¹
ç›®æ ‡ï¼šçˆ¬å–ä¸­å›½æ‰€æœ‰çŸ¥åæ™¯ç‚¹
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from typing import List, Dict
from urllib.parse import quote


class ChinaAttractionCrawler:
    """ä¸­å›½æ™¯ç‚¹çˆ¬è™«"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.attractions = []
        self.attraction_names = set()  # ç”¨äºå»é‡
    
    def crawl_china_world_heritage(self):
        """çˆ¬å–ä¸­å›½ä¸–ç•Œé—äº§"""
        print("ğŸ“ æ­£åœ¨çˆ¬å–ä¸­å›½ä¸–ç•Œé—äº§...")
        
        url = "https://zh.wikipedia.org/wiki/ä¸­å›½ä¸–ç•Œé—äº§åˆ—è¡¨"
        
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
            tables = soup.find_all('table', class_='wikitable')
            
            for table in tables:
                rows = table.find_all('tr')[1:]
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        # æå–æ™¯ç‚¹åç§°
                        name_cell = cells[0] if len(cells) > 0 else None
                        if name_cell:
                            name_link = name_cell.find('a')
                            if name_link:
                                name = name_link.get_text(strip=True)
                            else:
                                name = name_cell.get_text(strip=True)
                            
                            # æå–çœä»½/åœ°åŒº
                            province = ""
                            if len(cells) >= 3:
                                province = cells[2].get_text(strip=True)
                            
                            if name and name not in self.attraction_names:
                                self.attraction_names.add(name)
                                self.attractions.append({
                                    "name": name,
                                    "province": province,
                                    "country": "ä¸­å›½",
                                    "source": "ä¸–ç•Œé—äº§"
                                })
            
            print(f"  âœ“ ä¸­å›½ä¸–ç•Œé—äº§: {len([a for a in self.attractions if a['source'] == 'ä¸–ç•Œé—äº§'])} ä¸ª")
            
        except Exception as e:
            print(f"  âœ— ä¸–ç•Œé—äº§çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_5a_scenic_areas(self):
        """çˆ¬å–ä¸­å›½5Açº§æ™¯åŒº"""
        print("ğŸ“ æ­£åœ¨çˆ¬å–5Açº§æ™¯åŒº...")
        
        url = "https://zh.wikipedia.org/wiki/å›½å®¶5Açº§æ—…æ¸¸æ™¯åŒº"
        
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
            tables = soup.find_all('table', class_='wikitable')
            
            for table in tables:
                rows = table.find_all('tr')[1:]
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        # æå–æ™¯ç‚¹åç§°ï¼ˆé€šå¸¸åœ¨ç¬¬2åˆ—æˆ–ç¬¬3åˆ—ï¼‰
                        name = ""
                        province = ""
                        
                        for idx, cell in enumerate(cells):
                            text = cell.get_text(strip=True)
                            link = cell.find('a')
                            
                            # æ™¯ç‚¹åç§°é€šå¸¸æœ‰é“¾æ¥
                            if link and len(text) > 2 and idx >= 1:
                                if not name:
                                    name = text
                            
                            # çœä»½ä¿¡æ¯
                            if any(p in text for p in ['çœ', 'å¸‚', 'è‡ªæ²»åŒº', 'ç‰¹åˆ«è¡Œæ”¿åŒº']):
                                province = text
                        
                        if name and name not in self.attraction_names:
                            self.attraction_names.add(name)
                            self.attractions.append({
                                "name": name,
                                "province": province,
                                "country": "ä¸­å›½",
                                "source": "5Açº§æ™¯åŒº"
                            })
            
            print(f"  âœ“ 5Açº§æ™¯åŒº: {len([a for a in self.attractions if a['source'] == '5Açº§æ™¯åŒº'])} ä¸ª")
            
        except Exception as e:
            print(f"  âœ— 5Açº§æ™¯åŒºçˆ¬å–å¤±è´¥: {e}")
    
    def crawl_provincial_attractions(self):
        """çˆ¬å–å„çœæ—…æ¸¸æ™¯ç‚¹"""
        print("ğŸ“ æ­£åœ¨çˆ¬å–å„çœæ—…æ¸¸æ™¯ç‚¹...")
        
        # ä¸­å›½å„çœä»½å’Œç›´è¾–å¸‚
        provinces = [
            "åŒ—äº¬å¸‚", "ä¸Šæµ·å¸‚", "å¤©æ´¥å¸‚", "é‡åº†å¸‚",
            "æ²³åŒ—çœ", "å±±è¥¿çœ", "è¾½å®çœ", "å‰æ—çœ", "é»‘é¾™æ±Ÿçœ",
            "æ±Ÿè‹çœ", "æµ™æ±Ÿçœ", "å®‰å¾½çœ", "ç¦å»ºçœ", "æ±Ÿè¥¿çœ", "å±±ä¸œçœ",
            "æ²³å—çœ", "æ¹–åŒ—çœ", "æ¹–å—çœ", "å¹¿ä¸œçœ", "æµ·å—çœ",
            "å››å·çœ", "è´µå·çœ", "äº‘å—çœ", "é™•è¥¿çœ", "ç”˜è‚ƒçœ", "é’æµ·çœ",
            "å†…è’™å¤è‡ªæ²»åŒº", "å¹¿è¥¿å£®æ—è‡ªæ²»åŒº", "è¥¿è—è‡ªæ²»åŒº", "å®å¤å›æ—è‡ªæ²»åŒº", "æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº",
            "é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒº", "æ¾³é—¨ç‰¹åˆ«è¡Œæ”¿åŒº", "å°æ¹¾çœ"
        ]
        
        for province in provinces:
            try:
                # å°è¯•æ—…æ¸¸é¡µé¢
                url = f"https://zh.wikipedia.org/wiki/{province}æ—…æ¸¸"
                response = self.session.get(url, timeout=10)
                
                if response.status_code != 200:
                    # å°è¯•ä¸»é¡µé¢
                    url = f"https://zh.wikipedia.org/wiki/{province}"
                    response = self.session.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # æŸ¥æ‰¾æ—…æ¸¸ã€æ™¯ç‚¹ç›¸å…³ç« èŠ‚
                    for heading in soup.find_all(['h2', 'h3', 'h4']):
                        heading_text = heading.get_text().lower()
                        
                        if any(keyword in heading_text for keyword in [
                            'æ—…æ¸¸', 'æ™¯ç‚¹', 'åèƒœ', 'å¤è¿¹', 'é£æ™¯', 'é—äº§', 'å…¬å›­', 'å¯ºåº™', 'åšç‰©é¦†'
                        ]):
                            # è·å–è¯¥ç« èŠ‚ä¸‹çš„åˆ—è¡¨
                            next_element = heading.find_next_sibling()
                            
                            while next_element and next_element.name not in ['h2', 'h3', 'h4']:
                                if next_element.name in ['ul', 'ol']:
                                    items = next_element.find_all('li')
                                    
                                    for item in items:
                                        link = item.find('a')
                                        if link:
                                            name = link.get_text(strip=True)
                                            
                                            if (name and len(name) > 1 and 
                                                name not in self.attraction_names and
                                                not name.startswith(('ç¼–è¾‘', 'å‚è€ƒ', 'ç»´åŸº'))):
                                                
                                                self.attraction_names.add(name)
                                                self.attractions.append({
                                                    "name": name,
                                                    "province": province,
                                                    "country": "ä¸­å›½",
                                                    "source": f"{province}æ™¯ç‚¹"
                                                })
                                
                                next_element = next_element.find_next_sibling()
                
                time.sleep(0.5)
                
            except Exception as e:
                pass
        
        print(f"  âœ“ å„çœæ™¯ç‚¹: {len([a for a in self.attractions if 'æ™¯ç‚¹' in a.get('source', '')])} ä¸ª")
    
    def crawl_famous_mountains(self):
        """çˆ¬å–ä¸­å›½åå±±"""
        print("ğŸ“ æ­£åœ¨çˆ¬å–ä¸­å›½åå±±...")
        
        mountain_lists = [
            "ä¸­å›½äº”å²³",
            "ä¸­å›½å››å¤§ä½›æ•™åå±±",
            "ä¸­å›½å››å¤§é“æ•™åå±±"
        ]
        
        for mountain_list in mountain_lists:
            try:
                url = f"https://zh.wikipedia.org/wiki/{mountain_list}"
                response = self.session.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                    content = soup.find('div', id='mw-content-text')
                    if content:
                        links = content.find_all('a', href=True)
                        
                        for link in links:
                            text = link.get_text(strip=True)
                            
                            if (text and 'å±±' in text and len(text) <= 10 and
                                text not in self.attraction_names and
                                not text.startswith(('ç¼–è¾‘', 'å‚è€ƒ'))):
                                
                                self.attraction_names.add(text)
                                self.attractions.append({
                                    "name": text,
                                    "province": "",
                                    "country": "ä¸­å›½",
                                    "source": "åå±±"
                                })
                
                time.sleep(0.3)
                
            except Exception as e:
                pass
        
        print(f"  âœ“ åå±±: {len([a for a in self.attractions if a['source'] == 'åå±±'])} ä¸ª")
    
    def crawl_ancient_towns(self):
        """çˆ¬å–ä¸­å›½å¤é•‡"""
        print("ğŸ“ æ­£åœ¨çˆ¬å–ä¸­å›½å¤é•‡...")
        
        try:
            url = "https://zh.wikipedia.org/wiki/ä¸­å›½å†å²æ–‡åŒ–åé•‡"
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
                tables = soup.find_all('table', class_='wikitable')
                
                for table in tables:
                    rows = table.find_all('tr')[1:]
                    
                    for row in rows:
                        cells = row.find_all(['td', 'th'])
                        
                        if len(cells) >= 1:
                            name_cell = cells[0]
                            link = name_cell.find('a')
                            
                            if link:
                                name = link.get_text(strip=True)
                            else:
                                name = name_cell.get_text(strip=True)
                            
                            province = ""
                            if len(cells) >= 2:
                                province = cells[1].get_text(strip=True)
                            
                            if name and name not in self.attraction_names:
                                self.attraction_names.add(name)
                                self.attractions.append({
                                    "name": name,
                                    "province": province,
                                    "country": "ä¸­å›½",
                                    "source": "å†å²æ–‡åŒ–åé•‡"
                                })
            
            print(f"  âœ“ å¤é•‡: {len([a for a in self.attractions if a['source'] == 'å†å²æ–‡åŒ–åé•‡'])} ä¸ª")
            
        except Exception as e:
            print(f"  âœ— å¤é•‡çˆ¬å–å¤±è´¥: {e}")
    
    def save_results(self, filename="china_attractions.json"):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.attractions, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… å·²ä¿å­˜ {len(self.attractions)} ä¸ªæ™¯ç‚¹åˆ° {filename}")
    
    def print_summary(self):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        print(f"\n{'='*80}")
        print(f"çˆ¬å–ç»“æœç»Ÿè®¡")
        print(f"{'='*80}")
        print(f"æ€»æ™¯ç‚¹æ•°: {len(self.attractions)}")
        
        # æŒ‰æ¥æºç»Ÿè®¡
        sources = {}
        for attr in self.attractions:
            source = attr.get('source', 'æœªçŸ¥')
            sources[source] = sources.get(source, 0) + 1
        
        print(f"\næŒ‰æ¥æºåˆ†å¸ƒ:")
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            print(f"  {source}: {count} ä¸ª")
        
        # æŒ‰çœä»½ç»Ÿè®¡
        provinces = {}
        for attr in self.attractions:
            province = attr.get('province', 'æœªçŸ¥')
            if province:
                provinces[province] = provinces.get(province, 0) + 1
        
        if provinces:
            print(f"\næŒ‰çœä»½åˆ†å¸ƒï¼ˆå‰10ï¼‰:")
            for province, count in sorted(provinces.items(), key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {province}: {count} ä¸ª")
        
        print(f"\n{'='*80}\n")
    
    def print_table_sample(self, limit=50):
        """æ‰“å°å‰Nä¸ªæ™¯ç‚¹çš„è¡¨æ ¼"""
        print(f"\n{'='*100}")
        print(f"{'åºå·':<6}{'æ™¯ç‚¹åç§°':<35}{'çœä»½':<20}{'æ¥æº':<25}")
        print(f"{'='*100}")
        
        for idx, attraction in enumerate(self.attractions[:limit], 1):
            name = attraction['name'][:33]
            province = attraction.get('province', '')[:18]
            source = attraction.get('source', '')[:23]
            print(f"{idx:<6}{name:<35}{province:<20}{source:<25}")
        
        if len(self.attractions) > limit:
            print(f"... (è¿˜æœ‰ {len(self.attractions) - limit} ä¸ªæ™¯ç‚¹æœªæ˜¾ç¤º)")
        
        print(f"{'='*100}\n")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ‡¨ğŸ‡³ ä¸­å›½æ™¯ç‚¹çˆ¬è™«å¯åŠ¨...\n")
    
    crawler = ChinaAttractionCrawler()
    
    # æ‰§è¡Œå„é¡¹çˆ¬å–ä»»åŠ¡
    crawler.crawl_china_world_heritage()
    crawler.crawl_5a_scenic_areas()
    crawler.crawl_famous_mountains()
    crawler.crawl_ancient_towns()
    crawler.crawl_provincial_attractions()
    
    # æ˜¾ç¤ºç»Ÿè®¡
    crawler.print_summary()
    
    # æ˜¾ç¤ºå‰50ä¸ªæ™¯ç‚¹
    crawler.print_table_sample(50)
    
    # ä¿å­˜ç»“æœ
    crawler.save_results("china_attractions.json")
    
    print("âœ… çˆ¬å–å®Œæˆï¼")


if __name__ == "__main__":
    main()
